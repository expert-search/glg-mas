{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1ea2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fergu\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.corpora as corpora\n",
    "import gensim\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b76016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d43eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672e9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fergu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765270b3",
   "metadata": {},
   "source": [
    "### Lemmatize Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010e2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def func_lemmatize(words):\n",
    "    lemmatized = []\n",
    "    # Tag each word and \n",
    "    for word, tag in pos_tag(words):\n",
    "        wntag = tag[0].lower()\n",
    "        # Indlcude only adjectives, nouns verbs, adverbs (r) \n",
    "        wntag = wntag if wntag in ['a','r','n','v'] else None\n",
    "        # Lemmatize\n",
    "        lemma = wnl.lemmatize(word,wntag) if wntag else word\n",
    "        lemmatized.append(lemma)\n",
    "    return lemmatized\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "pattern = r'(\\w+)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eba502",
   "metadata": {},
   "source": [
    "### Other Lemmatizing Functions\n",
    "The following were attempts to make preprocessing faster, since this was found to be the primary efficiency bottleneck. None of these seemed to have the desired effect and the original lemmatizing approach (defined above) was ultimately used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf0cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_lemmatize(words):\n",
    "    allow = ['a','r','n','v']\n",
    "    \n",
    "    # condense all into a list comprehension to try to speed up preprocessing\n",
    "    list_2 = [wnl.lemmatize(word,wntag) if wntag else word for word, wntag in [(word,tag[0].lower() if tag[0].lower() in allow else None) for word,tag in pos_tag(words)]]\n",
    "    \n",
    "    return list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f13f7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_lemmatize(words):\n",
    "    allow = ['a','r','n','v']\n",
    "    # condense all two list comprehensions to try to speed up preprocessing\n",
    "    list_1 = [(word,tag[0].lower() if tag[0].lower() in allow else None) for word,tag in pos_tag(words)]\n",
    "    list_2 = [wnl.lemmatize(word,wntag) if wntag else word for word, wntag in list_1]\n",
    "    \n",
    "    return list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "801085be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_lemmatize(words):\n",
    "    allow = ['a','r','n','v']\n",
    "    # condense all two list comprehensions to try to speed up preprocessing\n",
    "    lemmatized = [wntag if wntag[0].lower() in allow for word,tag in pog_tag(words)]\n",
    "    lemma = [wnl.lemmatize(item) for item in list_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645ea79",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187581c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>article</th>\n",
       "      <th>url</th>\n",
       "      <th>section</th>\n",
       "      <th>publication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-09 18:31:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9</td>\n",
       "      <td>Lee Drutman</td>\n",
       "      <td>We should take concerns about the health of li...</td>\n",
       "      <td>This post is part of Polyarchy, an independent...</td>\n",
       "      <td>https://www.vox.com/polyarchy/2016/12/9/138983...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-10-07 21:26:46</td>\n",
       "      <td>2016</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7</td>\n",
       "      <td>Scott Davis</td>\n",
       "      <td>Colts GM Ryan Grigson says Andrew Luck's contr...</td>\n",
       "      <td>The Indianapolis Colts made Andrew Luck the h...</td>\n",
       "      <td>https://www.businessinsider.com/colts-gm-ryan-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Business Insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-26 00:00:00</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump denies report he ordered Mueller fired</td>\n",
       "      <td>DAVOS, Switzerland (Reuters) - U.S. President ...</td>\n",
       "      <td>https://www.reuters.com/article/us-davos-meeti...</td>\n",
       "      <td>Davos</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-06-27 00:00:00</td>\n",
       "      <td>2019</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France's Sarkozy reveals his 'Passions' but in...</td>\n",
       "      <td>PARIS (Reuters) - Former French president Nico...</td>\n",
       "      <td>https://www.reuters.com/article/france-politic...</td>\n",
       "      <td>World News</td>\n",
       "      <td>Reuters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-01-27 00:00:00</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Paris Hilton: Woman In Black For Uncle Monty's...</td>\n",
       "      <td>Paris Hilton arrived at LAX Wednesday dressed ...</td>\n",
       "      <td>https://www.tmz.com/2016/01/27/paris-hilton-mo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TMZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Unnamed: 0.1                 date  year  month day       author  \\\n",
       "0           0            0  2016-12-09 18:31:00  2016   12.0   9  Lee Drutman   \n",
       "1           1            1  2016-10-07 21:26:46  2016   10.0   7  Scott Davis   \n",
       "2           2            2  2018-01-26 00:00:00  2018    1.0  26          NaN   \n",
       "3           3            3  2019-06-27 00:00:00  2019    6.0  27          NaN   \n",
       "4           4            4  2016-01-27 00:00:00  2016    1.0  27          NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0  We should take concerns about the health of li...   \n",
       "1  Colts GM Ryan Grigson says Andrew Luck's contr...   \n",
       "2       Trump denies report he ordered Mueller fired   \n",
       "3  France's Sarkozy reveals his 'Passions' but in...   \n",
       "4  Paris Hilton: Woman In Black For Uncle Monty's...   \n",
       "\n",
       "                                             article  \\\n",
       "0  This post is part of Polyarchy, an independent...   \n",
       "1   The Indianapolis Colts made Andrew Luck the h...   \n",
       "2  DAVOS, Switzerland (Reuters) - U.S. President ...   \n",
       "3  PARIS (Reuters) - Former French president Nico...   \n",
       "4  Paris Hilton arrived at LAX Wednesday dressed ...   \n",
       "\n",
       "                                                 url     section  \\\n",
       "0  https://www.vox.com/polyarchy/2016/12/9/138983...         NaN   \n",
       "1  https://www.businessinsider.com/colts-gm-ryan-...         NaN   \n",
       "2  https://www.reuters.com/article/us-davos-meeti...       Davos   \n",
       "3  https://www.reuters.com/article/france-politic...  World News   \n",
       "4  https://www.tmz.com/2016/01/27/paris-hilton-mo...         NaN   \n",
       "\n",
       "        publication  \n",
       "0               Vox  \n",
       "1  Business Insider  \n",
       "2           Reuters  \n",
       "3           Reuters  \n",
       "4               TMZ  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "df = pd.read_csv('../all-the-news-2-1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf2fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null records and duplicates\n",
    "df = df.dropna(subset = ['article','title'])\n",
    "df.drop_duplicates(subset=['title'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d26202ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58d91c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2412021, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e4b47",
   "metadata": {},
   "source": [
    "### Approach 1: Filter out Irrelevant Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dd6b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'dropwords', i.e. words that any article should be dropped if they contain any of them\n",
    "# This is an attempt to filter out some of the more irrelevant content\n",
    "dropwords = ['fitch','stock','fidget','dakota','market']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b805527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude article \n",
    "excl = [ind for ind in df2.index if not any([word in df2.loc[ind,'title'] for word in dropwords])]\n",
    "new_ind = [ind for ind in df2.index if ind not in excl]\n",
    "df2 = df2.loc[new_ind,:]\n",
    "#df2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b391bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea7c6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['section'].isin(sections)]\n",
    "#df1_index(drop=True,inplace=True)\n",
    "#df_ht.drop(['Unnamed: 0','Unnamed: 0.1'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "191c66e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ht = pd.concat([df1,df2])\n",
    "df_ht.drop_duplicates(subset=['title'])\n",
    "df_ht.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "214f9548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(281411, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ht.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be0c7e",
   "metadata": {},
   "source": [
    "### Approach 2: Select a Desired Number of Records to Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43130ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 250000\n",
    "np.random.seed(3)\n",
    "inds_to_use = np.random.choice(df.index,num,replace=False)\n",
    "df2 = df.loc[inds_to_use,:]\n",
    "#df2 = df.loc[:num]\n",
    "df2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829c9d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3b9db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This function goes through the normal preprocessing steps on a pre-filtered dataset (see above) and performs the \n",
    "tokenizing, number- and special character-removal steps, lemmatizing, lower-case and stopword-removal steps.\n",
    "\n",
    "Sample input: \"We were going to go to the zoo at 09:00, but decided against it due to the rain!\"\n",
    "Output: ['go','go','zoo',decide,'against','due','rain']'''\n",
    "\n",
    "# Original version\n",
    "def clean(df):\n",
    "    start = time.time()\n",
    "    # Tokenize\n",
    "    df['article_words'] = [regexp_tokenize(article,pattern) for article in df['article']]\n",
    "    print('tokenizing done.')  \n",
    "    # Retain only alphabetical characters. Remove all punctuation, numerical and special characters\n",
    "    df['article_words'] = [[word for word in article if word.isalpha()] for article in df['article_words']]\n",
    "    print('isalpha done.')\n",
    "    # Lemmatize all tokens\n",
    "    df['article_words'] = [func_lemmatize(article) for article in df['article_words']]\n",
    "    print('lemmatizing done.')\n",
    "    # Set to lower case and remove stopwords\n",
    "    df['article_words'] = [[word.lower() for word in article if word.lower() not in stop] for article in df['article_words']]\n",
    "    interval = round((time.time() - start)/60,2)\n",
    "    print(f'That  took {interval} mins.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fff20dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing done.\n",
      "isalpha done.\n",
      "lemmatizing done.\n",
      "That  took 117.94 mins.\n"
     ]
    }
   ],
   "source": [
    "#5:05pm\n",
    "df_ht_clean = clean(df_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84c1c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ht_clean.to_csv('df_ht_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
